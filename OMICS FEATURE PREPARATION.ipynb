{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from lifelines import CoxPHFitter\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import entropy\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sksurv.datasets import load_breast_cancer\n",
    "from sksurv.linear_model import CoxPHSurvivalAnalysis, CoxnetSurvivalAnalysis\n",
    "from sksurv.preprocessing import OneHotEncoder\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.model_selection import GridSearchCV, KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from numbers import Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "cancers = ['BLCA', 'BRCA', 'HNSC', 'LAML', 'LGG', 'LUAD']\n",
    "path_prefix = 'C:/Users/sharony/SurvivalAnalysis/'\n",
    "if os.getlogin() =='meiry':\n",
    "    path_prefix = 'D:/sharon/medical_genomics_data/'\n",
    "\n",
    "min_feature_std = 0.00 #0.01\n",
    "# Drop features with low std "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calc_time_to_event(x):\n",
    "    #print(type(x) , x)\n",
    "    assert x.vital_status in ['Dead', 'Alive']\n",
    "    if x.vital_status=='Dead':\n",
    "        try:\n",
    "            assert isinstance(int(x.death_days_to), int)\n",
    "        except:\n",
    "            print(type(x.death_days_to), 'non int entry in x.death_days_to' , x.death_days_to, 'removing row',x.vital_status, x.death_days_to)        \n",
    "            return None\n",
    "        assert float(x.death_days_to) >= 0\n",
    "        return int(x.death_days_to)\n",
    "    try:\n",
    "        assert isinstance(int(x.last_contact_days_to), int) \n",
    "    except :\n",
    "        print(type(x.last_contact_days_to), 'non int entry in x.last_contact_days_to' , x.last_contact_days_to, 'removing row',x.vital_status, x.death_days_to)        \n",
    "        return None\n",
    "    if int(x.last_contact_days_to) < 0:\n",
    "        print('negative entry in x.last_contact_days_to' , x.last_contact_days_to, 'fixing it')\n",
    "    return abs(int(x.last_contact_days_to))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlated_features(ct, comment, omics, min_correlation_threshold = 0.9, max_features_to_correlate = None  ):\n",
    "    if max_features_to_correlate is None: \n",
    "        max_features_to_correlate = omics.shape[1]\n",
    "    assert(isinstance(max_features_to_correlate, int))\n",
    "    corr_matrix = omics.iloc[:,0:max_features_to_correlate-1].corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "    to_drop_corr = [column for column in upper.columns if any(upper[column] > min_correlation_threshold)]\n",
    "    print(datetime.now(), ct, comment,  'dropping highly correlated features', len(to_drop_corr))\n",
    "    cols_to_keep = [col not in to_drop_corr for col in omics.columns]\n",
    "    return cols_to_keep\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_omics(ct, omics_raw, clinical_data, comment):    \n",
    "    dup_idx = omics_raw.duplicated()\n",
    "    print(datetime.now(), ct, comment, f'will remove duplicate features {sum(dup_idx)}')\n",
    "    omics_raw = omics_raw.loc[~dup_idx]\n",
    "    omics_raw = omics_raw.transpose()\n",
    "    print(datetime.now(), ct, comment, omics_raw.shape)\n",
    "    omics_std = omics_raw.std(axis=0, skipna=True)\n",
    "    idx_omics_filter_std = omics_std <= min_feature_std\n",
    "    omics_raw = omics_raw.loc[:,~idx_omics_filter_std]\n",
    "    scaler = MinMaxScaler()\n",
    "    omics_scaled = pd.DataFrame(scaler.fit_transform(omics_raw), columns=omics_raw.columns)\n",
    "    T = clinical_data.apply(calc_time_to_event, axis = 1)\n",
    "    E = (clinical_data[\"vital_status\"] == 'Dead').astype(bool)\n",
    "    importances_E = mutual_info_classif(omics_scaled, E)\n",
    "    feature_importances_E = pd.Series(importances_E, omics_scaled.columns).sort_values()\n",
    "    drop_low_ig_E = list(feature_importances_E[feature_importances_E==0].keys())\n",
    "    print(datetime.now(), ct, comment, f'IG E:will drop {len(drop_low_ig_E)} features')\n",
    "    importances_T = mutual_info_classif(omics_scaled, T)\n",
    "    feature_importances_T = pd.Series(importances_T, omics_scaled.columns).sort_values()\n",
    "    drop_low_ig_T = list(feature_importances_T[feature_importances_T==0].keys())\n",
    "    print(datetime.now(), ct, comment, f'IG T:will drop {len(drop_low_ig_T)} features')\n",
    "    to_keep = [ (col not in drop_low_ig_T) and (col not in drop_low_ig_E) for col in omics_scaled.columns]\n",
    "    print(datetime.now(), ct, comment, sum(to_keep),len(to_keep))\n",
    "    omics = omics_scaled.loc[:, to_keep]\n",
    "    print(datetime.now(), ct, comment, omics.shape)\n",
    "    omics = omics.loc[:,correlated_features(ct, comment, omics)]\n",
    "    print(datetime.now(), ct, comment, 'remaining features', omics.shape[1])\n",
    "    return omics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-11 20:47:06.559752 BLCA ENTERED\n",
      "negative entry in x.last_contact_days_to -21 fixing it\n",
      "negative entry in x.last_contact_days_to -64 fixing it\n",
      "2020-12-11 20:47:09.052080 BLCA merged\n",
      "2020-12-11 20:47:09.052080 skipping merged BLCA_omics.pkl\n",
      "2020-12-11 20:47:09.053079 BLCA exp\n",
      "2020-12-11 20:47:09.053079 skipping exp BLCA_exp_omics.pkl\n",
      "2020-12-11 20:47:09.053079 BLCA methy\n",
      "2020-12-11 20:47:09.053079 skipping methy BLCA_methy_omics.pkl\n",
      "2020-12-11 20:47:09.054083 BLCA mirna\n",
      "2020-12-11 20:47:09.055105 skipping mirna BLCA_mirna_omics.pkl\n",
      "2020-12-11 20:47:09.058090 BLCA DONE\n",
      "2020-12-11 20:47:09.059063 BRCA ENTERED\n",
      "negative entry in x.last_contact_days_to -7 fixing it\n",
      "2020-12-11 20:47:12.897789 BRCA merged\n",
      "2020-12-11 20:47:12.898787 skipping merged BRCA_omics.pkl\n",
      "2020-12-11 20:47:12.898787 BRCA exp\n",
      "2020-12-11 20:47:12.899785 skipping exp BRCA_exp_omics.pkl\n",
      "2020-12-11 20:47:12.899785 BRCA methy\n",
      "2020-12-11 20:47:12.899785 skipping methy BRCA_methy_omics.pkl\n",
      "2020-12-11 20:47:12.900782 BRCA mirna\n",
      "2020-12-11 20:47:12.900782 skipping mirna BRCA_mirna_omics.pkl\n",
      "2020-12-11 20:47:12.905768 BRCA DONE\n",
      "2020-12-11 20:47:12.906766 HNSC ENTERED\n",
      "negative entry in x.last_contact_days_to -2 fixing it\n",
      "<class 'str'> non int entry in x.last_contact_days_to [Completed] removing row Alive [Not Applicable]\n",
      "2020-12-11 20:47:15.716248 HNSC removing invalid rows 1\n",
      "2020-12-11 20:47:15.852882 HNSC merged\n",
      "2020-12-11 20:47:15.853883 skipping merged HNSC_omics.pkl\n",
      "2020-12-11 20:47:15.853883 HNSC exp\n",
      "2020-12-11 20:47:15.853883 skipping exp HNSC_exp_omics.pkl\n",
      "2020-12-11 20:47:15.853883 HNSC methy\n",
      "2020-12-11 20:47:15.853883 skipping methy HNSC_methy_omics.pkl\n",
      "2020-12-11 20:47:15.854882 HNSC mirna\n",
      "2020-12-11 20:47:15.854882 skipping mirna HNSC_mirna_omics.pkl\n",
      "2020-12-11 20:47:15.861008 HNSC DONE\n",
      "2020-12-11 20:47:15.862004 LAML ENTERED\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "2020-12-11 20:47:16.978018 LAML removing invalid rows 7\n",
      "2020-12-11 20:47:17.033867 LAML merged\n",
      "2020-12-11 20:47:17.034865 skipping merged LAML_omics.pkl\n",
      "2020-12-11 20:47:17.035865 LAML exp\n",
      "2020-12-11 20:47:17.036868 skipping exp LAML_exp_omics.pkl\n",
      "2020-12-11 20:47:17.037863 LAML methy\n",
      "2020-12-11 20:47:17.038854 skipping methy LAML_methy_omics.pkl\n",
      "2020-12-11 20:47:17.038854 LAML mirna\n",
      "2020-12-11 20:47:17.038854 skipping mirna LAML_mirna_omics.pkl\n",
      "2020-12-11 20:47:17.041848 LAML DONE\n",
      "2020-12-11 20:47:17.041848 LGG ENTERED\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "<class 'str'> non int entry in x.last_contact_days_to [Discrepancy] removing row Alive [Not Applicable]\n",
      "2020-12-11 20:47:20.652187 LGG removing invalid rows 1\n",
      "2020-12-11 20:47:20.816744 LGG merged\n",
      "2020-12-11 20:47:21.647522 LGG merged will remove duplicate features 1978\n",
      "2020-12-11 20:47:21.733292 LGG merged (380, 40422)\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "2020-12-11 20:50:37.350808 LGG merged IG E:will drop 12682 features\n",
      "2020-12-11 21:11:28.469198 LGG merged IG T:will drop 22651 features\n",
      "2020-12-11 21:11:41.963909 LGG merged 11973 40421\n",
      "2020-12-11 21:11:42.012777 LGG merged (380, 11973)\n",
      "2020-12-11 21:14:44.865502 LGG merged dropping highly correlated features 1279\n",
      "2020-12-11 21:14:45.564622 LGG merged remaining features 10694\n",
      "2020-12-11 21:14:45.727187 LGG exp\n",
      "2020-12-11 21:14:46.252773 LGG exp will remove duplicate features 343\n",
      "2020-12-11 21:14:46.302642 LGG exp (380, 20187)\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "2020-12-11 21:16:32.027068 LGG exp IG E:will drop 7761 features\n",
      "2020-12-11 21:27:00.590431 LGG exp IG T:will drop 11490 features\n",
      "2020-12-11 21:27:05.480290 LGG exp 5296 20186\n",
      "2020-12-11 21:27:05.499215 LGG exp (380, 5296)\n",
      "2020-12-11 21:27:38.267365 LGG exp dropping highly correlated features 87\n",
      "2020-12-11 21:27:38.334183 LGG exp remaining features 5209\n",
      "2020-12-11 21:27:38.367098 LGG methy\n",
      "2020-12-11 21:27:38.840860 LGG methy will remove duplicate features 1271\n",
      "2020-12-11 21:27:38.884706 LGG methy (380, 18729)\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "2020-12-11 21:29:13.929502 LGG methy IG E:will drop 4213 features\n",
      "2020-12-11 21:38:09.980894 LGG methy IG T:will drop 10422 features\n",
      "2020-12-11 21:38:13.699912 LGG methy 6277 18729\n",
      "2020-12-11 21:38:13.722850 LGG methy (380, 6277)\n",
      "2020-12-11 21:39:00.687157 LGG methy dropping highly correlated features 1182\n",
      "2020-12-11 21:39:00.955415 LGG methy remaining features 5095\n",
      "2020-12-11 21:39:00.986366 LGG mirna\n",
      "2020-12-11 21:39:01.095041 LGG mirna will remove duplicate features 363\n",
      "2020-12-11 21:39:01.100028 LGG mirna (380, 1507)\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "negative entry in x.last_contact_days_to -1 fixing it\n",
      "2020-12-11 21:39:08.513127 LGG mirna IG E:will drop 716 features\n",
      "2020-12-11 21:39:45.958602 LGG mirna IG T:will drop 778 features\n",
      "2020-12-11 21:39:45.983534 LGG mirna 394 1506\n",
      "2020-12-11 21:39:45.985531 LGG mirna (380, 394)\n",
      "2020-12-11 21:39:46.223925 LGG mirna dropping highly correlated features 14\n",
      "2020-12-11 21:39:46.226880 LGG mirna remaining features 380\n",
      "2020-12-11 21:39:46.232866 LGG DONE\n",
      "2020-12-11 21:39:46.232866 LUAD ENTERED\n",
      "<class 'str'> non int entry in x.last_contact_days_to [Discrepancy] removing row Alive [Not Applicable]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "<class 'str'> non int entry in x.death_days_to [Not Available] removing row Dead [Not Available]\n",
      "2020-12-11 21:39:49.267036 LUAD removing invalid rows 5\n",
      "2020-12-11 21:39:49.409654 LUAD merged\n",
      "2020-12-11 21:39:50.065893 LUAD merged will remove duplicate features 2041\n",
      "2020-12-11 21:39:50.127753 LUAD merged (313, 40359)\n",
      "2020-12-11 21:42:41.969403 LUAD merged IG E:will drop 19958 features\n",
      "2020-12-11 22:01:11.983725 LUAD merged IG T:will drop 25038 features\n",
      "2020-12-11 22:01:25.029716 LUAD merged 7639 40358\n",
      "2020-12-11 22:01:25.065619 LUAD merged (313, 7639)\n",
      "2020-12-11 22:02:48.492922 LUAD merged dropping highly correlated features 297\n",
      "2020-12-11 22:02:48.663465 LUAD merged remaining features 7342\n",
      "2020-12-11 22:02:48.730285 LUAD exp\n",
      "2020-12-11 22:02:49.396498 LUAD exp will remove duplicate features 404\n",
      "2020-12-11 22:02:49.461327 LUAD exp (313, 20126)\n",
      "2020-12-11 22:04:55.065936 LUAD exp IG E:will drop 10305 features\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    \n",
    "for ct in cancers:\n",
    "    # Loading raw data\n",
    "    print(datetime.now(), ct, 'ENTERED')\n",
    "\n",
    "    ct_path = f'{path_prefix}{ct}'\n",
    "    clinical_data = pd.read_table(ct_path+'/clinical')\n",
    "    exp_data_raw = np.log2(1+pd.read_table(ct_path+'/exp'))\n",
    "    methy_data_raw = pd.read_table(ct_path+'/methy')\n",
    "    mirna_data_raw = np.log2(1+pd.read_table(ct_path+'/mirna'))\n",
    "    T = clinical_data.apply(calc_time_to_event, axis = 1)\n",
    "    valid_T_keys = list(T[~T.isna()].index)\n",
    "    # Merging \n",
    "    omics_raw = pd.concat([exp_data_raw, methy_data_raw, mirna_data_raw], axis=0, ignore_index=True)\n",
    "    if len(valid_T_keys) < len(T):\n",
    "        print(datetime.now(), ct, 'removing invalid rows', len(T) - len(valid_T_keys))\n",
    "        omics_raw = omics_raw.iloc[:,valid_T_keys]\n",
    "        exp_data_raw = exp_data_raw.iloc[:,valid_T_keys]\n",
    "        methy_data_raw = methy_data_raw.iloc[:,valid_T_keys]\n",
    "        mirna_data_raw = mirna_data_raw.iloc[:,valid_T_keys]\n",
    "        clinical_data = clinical_data.iloc[valid_T_keys,:]\n",
    "    print(datetime.now(), ct, 'merged')\n",
    "    if os.path.exists(f'{ct}_omics.pkl'):\n",
    "        print(datetime.now(), 'skipping', 'merged', f'{ct}_omics.pkl')\n",
    "    else:\n",
    "        omics = process_omics(ct, omics_raw, clinical_data, 'merged')\n",
    "        pickle.dump(omics, open(f'{ct}_omics.pkl', 'wb')) \n",
    "    print(datetime.now(), ct, 'exp')\n",
    "    if os.path.exists(f'{ct}_exp_omics.pkl'):\n",
    "        print(datetime.now(), 'skipping', 'exp', f'{ct}_exp_omics.pkl')\n",
    "    else:\n",
    "        exp = process_omics(ct, exp_data_raw, clinical_data, 'exp')\n",
    "        pickle.dump(exp, open(f'{ct}_exp_omics.pkl', 'wb'))\n",
    "    print(datetime.now(), ct, 'methy')\n",
    "    if os.path.exists(f'{ct}_methy_omics.pkl'):\n",
    "        print(datetime.now(), 'skipping', 'methy', f'{ct}_methy_omics.pkl')\n",
    "    else:\n",
    "        methy = process_omics(ct, methy_data_raw, clinical_data, 'methy')\n",
    "        pickle.dump(methy, open(f'{ct}_methy_omics.pkl', 'wb'))\n",
    "    print(datetime.now(), ct, 'mirna')\n",
    "    if os.path.exists(f'{ct}_mirna_omics.pkl'):\n",
    "        print(datetime.now(), 'skipping', 'mirna', f'{ct}_mirna_omics.pkl')\n",
    "    else:\n",
    "        mirna = process_omics(ct, mirna_data_raw, clinical_data, 'mirna')\n",
    "        pickle.dump(mirna, open(f'{ct}_mirna_omics.pkl', 'wb'))\n",
    "    pickle.dump(clinical_data, open(f'{ct}_clinical.pkl', 'wb'))\n",
    "    print(datetime.now(), ct, 'DONE')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_prefix = 'C:/Users/sharony/SurvivalAnalysis/'\n",
    "if os.getlogin() =='meiry':\n",
    "    path_prefix = 'D:/sharon/medical_genomics_data/'\n",
    "clinical_data = pd.read_table(path_prefix+ 'BLCA/clinical')\n",
    "\n",
    "exp_data_raw = np.log2(1+pd.read_table(path_prefix+ 'BLCA/exp'))\n",
    "methy_data_raw = pd.read_table(path_prefix+ 'BLCA/methy')\n",
    "mirna_data_raw = np.log2(1+pd.read_table(path_prefix+ 'BLCA/mirna'))\n",
    "omics_raw = pd.concat([exp_data_raw, methy_data_raw, mirna_data_raw], axis=0, ignore_index=True)\n",
    "dup_idx = omics_raw.duplicated()\n",
    "print(f'will remove duplicate features{sum(dup_idx)}')\n",
    "omics_raw = omics_raw.loc[~dup_idx]\n",
    "omics_raw = omics_raw.transpose()\n",
    "print(omics_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "T = clinical_data.apply(calc_time_to_event, axis = 1).astype('int32')\n",
    "E = (clinical_data[\"vital_status\"] == 'Dead').astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features_to_correlate = 5000 #50000\n",
    "min_correlation_threshold = 0.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before filtering low std\n",
      " count    40258.000000\n",
      "mean         0.679650\n",
      "std          0.635853\n",
      "min          0.000000\n",
      "25%          0.249385\n",
      "50%          0.321139\n",
      "75%          0.917353\n",
      "max          5.262542\n",
      "dtype: float64\n",
      "will filer those with std below 0.0\n",
      "original features 40258\n",
      "# of features to remove (std<=0.0): 1\n",
      "after filtering low std\n",
      " count    40257.000000\n",
      "mean         0.679667\n",
      "std          0.635852\n",
      "min          0.003800\n",
      "25%          0.249389\n",
      "50%          0.321146\n",
      "75%          0.917365\n",
      "max          5.262542\n",
      "dtype: float64\n",
      "(304, 40257)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "omics_std = omics_raw.std(axis=0, skipna=True)\n",
    "print('before filtering low std\\n',omics_std.describe())\n",
    "print(f'will filer those with std below {min_feature_std}')\n",
    "idx_omics_filter_std = omics_std <= min_feature_std # boolean vector describing std == 0\n",
    "print('original features', omics_raw.shape[1])\n",
    "print(f'# of features to remove (std<={min_feature_std }):', sum(idx_omics_filter_std))\n",
    "omics_std = omics_raw.std(axis=0, skipna=True)\n",
    "print('after filtering low std\\n',omics_std.describe())\n",
    "print(omics_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(304, 40257)\n"
     ]
    }
   ],
   "source": [
    "scaler = MinMaxScaler()\n",
    "omics_scaled = pd.DataFrame(scaler.fit_transform(omics_raw), columns=omics_raw.columns)\n",
    "\n",
    "print(omics_scaled.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_time_to_event(x):\n",
    "    #print(type(x) , x)\n",
    "    assert x.vital_status in ['Dead', 'Alive']\n",
    "    if x.vital_status=='Dead':\n",
    "        return x.death_days_to\n",
    "    return x.last_contact_days_to\n",
    "\n",
    "T = clinical_data.apply(calc_time_to_event, axis = 1).astype('int32')\n",
    "E = (clinical_data[\"vital_status\"] == 'Dead').astype(bool)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# entropy_T = entropy(T.value_counts(dropna=True, normalize=True).values)\n",
    "# entropy_E = entropy(E.value_counts(dropna=True, normalize=True).values)\n",
    "# print(entropy_E, entropy_T)\n",
    "# min_ig_E_threshold = 0.1 * entropy_E\n",
    "# min_ig_T_threshold = 0.1 * entropy_T\n",
    "# print(min_ig_E_threshold, min_ig_T_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG E:will drop 18670 features\n"
     ]
    }
   ],
   "source": [
    "importances_E = mutual_info_classif(omics_scaled, E)\n",
    "feature_importances_E = pd.Series(importances_E, omics_scaled.columns).sort_values()\n",
    "drop_low_ig_E = list(feature_importances_E[feature_importances_E==0].keys())\n",
    "print(f'IG E:will drop {len(drop_low_ig_E)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IG T:will drop 22460 features\n"
     ]
    }
   ],
   "source": [
    "importances_T = mutual_info_classif(omics_scaled, T)\n",
    "feature_importances_T = pd.Series(importances_T, omics_scaled.columns).sort_values()\n",
    "drop_low_ig_T = list(feature_importances_T[feature_importances_T==0].keys())\n",
    "print(f'IG T:will drop {len(drop_low_ig_T)} features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9780 40257\n"
     ]
    }
   ],
   "source": [
    "to_keep = [ (col not in drop_low_ig_T) and (col not in drop_low_ig_E) for col in omics_scaled.columns]\n",
    "print(sum(to_keep),len(to_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 9780)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omics = omics_scaled.loc[:, to_keep]\n",
    "omics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original features 21568\n",
      "# of features to remove (std<=0.1): 654\n",
      "remaining features 20914\n",
      "count    20914.000000\n",
      "mean         0.223481\n",
      "std          0.063968\n",
      "min          0.100153\n",
      "25%          0.163123\n",
      "50%          0.241957\n",
      "75%          0.279063\n",
      "max          0.449797\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# idx_omics_filter_std = omics_std <= min_feature_std # boolean vector describing std == 0\n",
    "# print('original features', omics.shape[1])\n",
    "# print(f'# of features to remove (std<={min_feature_std }):', sum(idx_omics_filter_std))\n",
    "# omics = omics.loc[:,~idx_omics_filter_std]\n",
    "# omics_std = omics.std(axis=0, skipna=True)\n",
    "# print('remaining features', omics.shape[1])\n",
    "# print(omics_std.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create correlation matrix\n",
    "max_features_to_correlate = omics.shape[1]\n",
    "min_correlation_threshold = 0.9\n",
    "corr_matrix = omics.iloc[:,0:max_features_to_correlate-1].corr().abs()\n",
    "# Select upper triangle of correlation matrix\n",
    "upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "# Find index of feature columns with correlation greater than 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 16:56:01.057313 dropping highly correlated features 391\n"
     ]
    }
   ],
   "source": [
    "to_drop_corr = [column for column in upper.columns if any(upper[column] > min_correlation_threshold)]\n",
    "print(datetime.now(), 'dropping highly correlated features', len(to_drop_corr))\n",
    "cols_to_keep = [col not in to_drop_corr for col in omics.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pickle.dump(upper, open('upper.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = pickle.load(open(\"upper.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(304, 9780)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "omics.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remaining features 9389\n"
     ]
    }
   ],
   "source": [
    "omics = omics.loc[:,cols_to_keep]\n",
    "print('remaining features', omics.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(omics, open('omics.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "omics = pickle.load(open(\"omics.pkl\", 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-12-10 16:56:22.015589\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "print(datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn_pandas import DataFrameMapper \n",
    "import torch # For building the networks \n",
    "import torchtuples as tt # Some useful functions\n",
    "from pycox.datasets import metabric\n",
    "from pycox.models import LogisticHazard\n",
    "# from pycox.models import PMF\n",
    "# from pycox.models import DeepHitSingle\n",
    "from pycox.evaluation import EvalSurv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data = omics.copy()\n",
    "data.loc[:,'time_to_event'] = clinical_data.apply(calc_time_to_event, axis = 1).astype('int32').values\n",
    "data.loc[:,'event'] = (clinical_data['vital_status'].values == 'Dead')\n",
    "\n",
    "data.replace([np.inf, -np.inf], np.nan)\n",
    "print('null values in data', data.isnull().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: norm_delta = 5.15222, step_size = 0.9000, log_lik = -382.58465, newton_decrement = 578.56939, seconds_since_start = 1029.2\n",
      "Iteration 2: norm_delta = 1.11734, step_size = 0.2250, log_lik = -250.65196, newton_decrement = 81.32051, seconds_since_start = 1897.2\n",
      "Iteration 3: norm_delta = 0.85129, step_size = 0.2925, log_lik = -209.85963, newton_decrement = 55.62084, seconds_since_start = 2465.4\n",
      "Iteration 4: norm_delta = 2.54494, step_size = 0.4943, log_lik = -171.39592, newton_decrement = 110.97268, seconds_since_start = 3007.8\n",
      "Iteration 5: norm_delta = 2.05560, step_size = 0.6298, log_lik = -135.14333, newton_decrement = 34.36561, seconds_since_start = 3538.1\n",
      "Iteration 6: norm_delta = 11.05935, step_size = 0.8023, log_lik = -104.47644, newton_decrement = 342.70510, seconds_since_start = 4060.7\n",
      "Iteration 7: norm_delta = 2.97630, step_size = 0.2205, log_lik = -102.94604, newton_decrement = 27.82815, seconds_since_start = 4598.4\n",
      "Iteration 8: norm_delta = 2.25695, step_size = 0.2809, log_lik = -89.38428, newton_decrement = 18.61789, seconds_since_start = 5119.6\n",
      "Iteration 9: norm_delta = 1.41407, step_size = 0.4747, log_lik = -75.51845, newton_decrement = 9.31027, seconds_since_start = 5639.0\n",
      "Iteration 10: norm_delta = 19.48484, step_size = 0.8023, log_lik = -90.91480, newton_decrement = 607.81094, seconds_since_start = 6177.2\n",
      "Iteration 11: norm_delta = 2.02275, step_size = 0.0882, log_lik = -71.95408, newton_decrement = 8.14078, seconds_since_start = 6700.2\n",
      "Iteration 12: norm_delta = 1.81149, step_size = 0.1124, log_lik = -70.22640, newton_decrement = 6.64009, seconds_since_start = 7228.6\n",
      "Iteration 13: norm_delta = 1.49581, step_size = 0.1899, log_lik = -67.94002, newton_decrement = 4.69095, seconds_since_start = 7777.3\n",
      "Iteration 14: norm_delta = 1.10927, step_size = 0.3209, log_lik = -65.39627, newton_decrement = 2.69342, seconds_since_start = 8299.0\n",
      "Iteration 15: norm_delta = 19.36753, step_size = 0.5424, log_lik = -75.88133, newton_decrement = 599.13264, seconds_since_start = 8825.6\n",
      "Iteration 16: norm_delta = 1.82647, step_size = 0.0691, log_lik = -68.07980, newton_decrement = 5.54617, seconds_since_start = 9383.2\n",
      "Iteration 17: norm_delta = 1.66849, step_size = 0.0880, log_lik = -67.14621, newton_decrement = 4.64890, seconds_since_start = 9994.9\n",
      "Iteration 18: norm_delta = 1.42354, step_size = 0.1488, log_lik = -65.86545, newton_decrement = 3.41986, seconds_since_start = 10628.7\n",
      "Iteration 19: norm_delta = 1.07120, step_size = 0.2514, log_lik = -64.36036, newton_decrement = 1.99284, seconds_since_start = 11579.2\n",
      "Iteration 20: norm_delta = 0.65464, step_size = 0.4249, log_lik = -66.73189, newton_decrement = 18.26663, seconds_since_start = 12493.3\n",
      "Iteration 21: norm_delta = 1.08610, step_size = 0.7181, log_lik = -64.02559, newton_decrement = 1.91411, seconds_since_start = 13446.0\n",
      "Iteration 22: norm_delta = 20.07131, step_size = 0.8820, log_lik = -99.15944, newton_decrement = 643.84884, seconds_since_start = 14350.8\n",
      "Iteration 23: norm_delta = 1.88324, step_size = 0.0882, log_lik = -68.45210, newton_decrement = 5.92845, seconds_since_start = 15264.1\n",
      "Iteration 24: norm_delta = 1.67240, step_size = 0.1124, log_lik = -67.19443, newton_decrement = 4.71756, seconds_since_start = 15975.7\n",
      "Iteration 25: norm_delta = 1.35640, step_size = 0.1899, log_lik = -65.57197, newton_decrement = 3.16416, seconds_since_start = 16555.8\n",
      "Iteration 26: norm_delta = 0.92661, step_size = 0.3209, log_lik = -63.86353, newton_decrement = 1.54912, seconds_since_start = 17137.1\n",
      "Iteration 27: norm_delta = 19.67310, step_size = 0.5424, log_lik = -78.99046, newton_decrement = 611.99791, seconds_since_start = 17705.1\n",
      "Iteration 28: norm_delta = 1.77745, step_size = 0.0691, log_lik = -67.42722, newton_decrement = 5.11923, seconds_since_start = 18298.2\n",
      "Iteration 29: norm_delta = 1.62138, step_size = 0.0880, log_lik = -66.56554, newton_decrement = 4.27711, seconds_since_start = 18886.8\n",
      "Iteration 30: norm_delta = 1.38094, step_size = 0.1488, log_lik = -65.38734, newton_decrement = 3.12921, seconds_since_start = 19430.7\n",
      "Iteration 31: norm_delta = 1.03596, step_size = 0.2514, log_lik = -64.01066, newton_decrement = 1.79833, seconds_since_start = 20034.2\n",
      "Iteration 32: norm_delta = 3.44762, step_size = 0.4249, log_lik = -68.41639, newton_decrement = 105.63427, seconds_since_start = 20588.8\n",
      "Iteration 33: norm_delta = 2.45789, step_size = 0.5413, log_lik = -71.87376, newton_decrement = 9.59599, seconds_since_start = 21191.4\n",
      "Iteration 34: norm_delta = 0.12819, step_size = 0.6897, log_lik = -63.18988, newton_decrement = 0.19216, seconds_since_start = 21820.7\n",
      "Iteration 35: norm_delta = 0.18288, step_size = 1.0000, log_lik = -64.64100, newton_decrement = 2.37309, seconds_since_start = 22364.6\n",
      "Iteration 36: norm_delta = 0.07517, step_size = 0.8820, log_lik = -62.99882, newton_decrement = 0.01771, seconds_since_start = 22890.7\n",
      "Iteration 37: norm_delta = 0.06189, step_size = 0.8820, log_lik = -62.97756, newton_decrement = 0.00597, seconds_since_start = 23418.7\n",
      "Iteration 38: norm_delta = 0.00034, step_size = 1.0000, log_lik = -62.97157, newton_decrement = 0.00000, seconds_since_start = 23965.1\n",
      "Iteration 39: norm_delta = 0.00000, step_size = 1.0000, log_lik = -62.97157, newton_decrement = 0.00000, seconds_since_start = 24494.5\n",
      "Convergence success after 39 iterations.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lifelines.CoxPHFitter: fitted with 304 total observations, 224 right-censored observations>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph = CoxPHFitter(penalizer = 0.01)\n",
    "cph.fit(data, duration_col='time_to_event', event_col='event', show_progress = True)\n",
    "#cph.print_summary(columns=data.columns[0:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#save to file\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('cph.pickle', 'wb') as handle:\n",
    "    pickle.dump(cph, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('cph.pickle', 'rb') as handle:\n",
    "    cph = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportional hazard assumption looks okay.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cph.check_assumptions(data, p_value_threshold=0.05, show_plots=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConvergenceError",
     "evalue": "delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConvergenceError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-62-c350467ba550>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlifelines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mk_fold_cross_validation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'time_to_event'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'event'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"concordance_index\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mcox_scores\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'cox_regular'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\utils\\__init__.py\u001b[0m in \u001b[0;36mk_fold_cross_validation\u001b[1;34m(fitters, df, duration_col, event_col, k, scoring_method, fitter_kwargs)\u001b[0m\n\u001b[0;32m    773\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfitter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfitter_scores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    774\u001b[0m             \u001b[1;31m# fit the fitter to the training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 775\u001b[1;33m             \u001b[0mfitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtraining_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mduration_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mduration_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevent_col\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevent_col\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfitter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    776\u001b[0m             \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfitter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtesting_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring_method\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\utils\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_censoring_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, df, duration_col, event_col, show_progress, initial_point, strata, step_size, weights_col, cluster_col, robust, batch_mode, timeline, formula, entry_col)\u001b[0m\n\u001b[0;32m    274\u001b[0m         \"\"\"\n\u001b[0;32m    275\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoalesce\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstrata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 276\u001b[1;33m         self._model = self._fit_model(\n\u001b[0m\u001b[0;32m    277\u001b[0m             \u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m             \u001b[0mduration_col\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36m_fit_model\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_fit_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    596\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseline_estimation_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"breslow\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 597\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_model_breslow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    598\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbaseline_estimation_method\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"spline\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    599\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit_model_spline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36m_fit_model_breslow\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    608\u001b[0m         )\n\u001b[0;32m    609\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCensoringType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_right_censoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m             \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\utils\\__init__.py\u001b[0m in \u001b[0;36mf\u001b[1;34m(model, *args, **kwargs)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m             \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_censoring_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRIGHT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, df, duration_col, event_col, show_progress, initial_point, strata, step_size, weights_col, cluster_col, robust, batch_mode, timeline, formula, entry_col)\u001b[0m\n\u001b[0;32m   1225\u001b[0m         )\n\u001b[0;32m   1226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1227\u001b[1;33m         params_, ll_, variance_matrix_, baseline_hazard_, baseline_cumulative_hazard_, model = self._fit_model(\n\u001b[0m\u001b[0;32m   1228\u001b[0m             \u001b[0mX_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m             \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36m_fit_model\u001b[1;34m(self, X, T, E, weights, entries, initial_point, step_size, show_progress)\u001b[0m\n\u001b[0;32m   1350\u001b[0m         \u001b[0mshow_progress\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1351\u001b[0m     ):\n\u001b[1;32m-> 1352\u001b[1;33m         beta_, ll_, hessian_ = self._newton_rhapson_for_efron_model(\n\u001b[0m\u001b[0;32m   1353\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_point\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_point\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstep_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_progress\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progress\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1354\u001b[0m         )\n",
      "\u001b[1;32mc:\\users\\meiry\\appdata\\local\\programs\\python\\python38\\lib\\site-packages\\lifelines\\fitters\\coxph_fitter.py\u001b[0m in \u001b[0;36m_newton_rhapson_for_efron_model\u001b[1;34m(self, X, T, E, weights, entries, initial_point, step_size, precision, show_progress, max_steps)\u001b[0m\n\u001b[0;32m   1516\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1517\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_values_post_fitting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1518\u001b[1;33m                 raise exceptions.ConvergenceError(\n\u001b[0m\u001b[0;32m   1519\u001b[0m                     \u001b[1;34m\"\"\"delta contains nan value(s). Convergence halted. {0}\"\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCONVERGENCE_DOCS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1520\u001b[0m                 )\n",
      "\u001b[1;31mConvergenceError\u001b[0m: delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation: https://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model"
     ]
    }
   ],
   "source": [
    "# Cross Validation\n",
    "cox_scores= {}\n",
    "\n",
    "from lifelines.utils import k_fold_cross_validation\n",
    "scores = k_fold_cross_validation(cph, data, duration_col='time_to_event', event_col='event', k=2, scoring_method=\"concordance_index\")\n",
    "print(scores)\n",
    "cox_scores['cox_regular'] = {}\n",
    "cox_scores['cox_regular'][cph]= scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cox_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cox with Regularizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_y = data[[ 'event','time_to_event']]\n",
    "\n",
    "y = df_y.to_numpy(copy=True)\n",
    "\n",
    "X = data.drop(columns=['event', 'time_to_event']).to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y2=([tuple(y[i,:]) for i in range(y.shape[0])])\n",
    "\n",
    "y2 = np.asarray([tuple(y[i,:]) for i in range(y.shape[0])],dtype=[('e.tdm', '?'), ('t.tdm', '<f8')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cox \n",
    "cox = CoxnetSurvivalAnalysis(l1_ratio=1.0, alpha_min_ratio=0.00, n_alphas = 1)\n",
    "cox.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = cox.alphas_\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "gcv = GridSearchCV(\n",
    "    make_pipeline(cox),\n",
    "    param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in alphas]},\n",
    "    cv=cv,\n",
    "    error_score=0.5,\n",
    "    n_jobs=4).fit(X, y2)\n",
    "\n",
    "cv_results_cox = pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cox_dict = dict(zip(cv_results_cox.param_coxnetsurvivalanalysis__alphas,cv_results_cox.mean_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ridge\n",
    "cox_ridge = CoxnetSurvivalAnalysis(l1_ratio=0.0, alpha_min_ratio=0.01,n_alphas = 50)\n",
    "cox_ridge.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = cox_ridge.alphas_\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "gcv = GridSearchCV(\n",
    "    make_pipeline(cox_ridge),\n",
    "    param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in alphas]},\n",
    "    cv=cv,\n",
    "    error_score=0.5,\n",
    "    n_jobs=4).fit(X, y2)\n",
    "\n",
    "cv_results_ridge = pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cox_ridge_dict = dict(zip(cv_results_ridge.param_coxnetsurvivalanalysis__alphas,cv_results_ridge.mean_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "cox_lasso = CoxnetSurvivalAnalysis(l1_ratio=1.0, alpha_min_ratio=0.01,n_alphas = 50)\n",
    "cox_lasso.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = cox_lasso.alphas_\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "gcv = GridSearchCV(\n",
    "    make_pipeline(cox_lasso),\n",
    "    param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in alphas]},\n",
    "    cv=cv,\n",
    "    error_score=0.5,\n",
    "    n_jobs=4).fit(X, y2)\n",
    "\n",
    "cv_results_lasso = pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_cox_lasso_dict = dict(zip(cv_results_lasso.param_coxnetsurvivalanalysis__alphas,cv_results_lasso.mean_test_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elastic Net\n",
    "cox_elastic_net = CoxnetSurvivalAnalysis(l1_ratio=0.5, alpha_min_ratio=0.01, n_alphas =50)\n",
    "cox_elastic_net.fit(X, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas = cox_elastic_net.alphas_\n",
    "cv = KFold(n_splits=3, shuffle=True, random_state=0)\n",
    "gcv = GridSearchCV(\n",
    "    make_pipeline(cox_elastic_net),\n",
    "    param_grid={\"coxnetsurvivalanalysis__alphas\": [[v] for v in alphas]},\n",
    "    cv=cv,\n",
    "    error_score=0.5,\n",
    "    n_jobs=4).fit(X, y2)\n",
    "\n",
    "cv_results_elastic_net = pd.DataFrame(gcv.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_elastic_net_dict = dict(zip(cv_results_elastic_net.param_coxnetsurvivalanalysis__alphas,cv_results_elastic_net.mean_test_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
